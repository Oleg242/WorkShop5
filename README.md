# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Кочешков Олег Романович
- РИ-230914
  
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

Вот, что у меня получилось.
![image](https://github.com/user-attachments/assets/5edfb72c-a686-4c7c-921b-59c512865ede)

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.
Простыми словами, корреляция - это степень взаимосвязи между двумя вещами или явлениями. Когда одно что-то меняется, другое тоже меняется в какой-то определенной степени. Я предпалогаю, что в скрипте “коэффициент корреляции” представлен в виде forceMultiplier. Он влияет на передвижение нашего агента, который должен достигнуть цели. То есть изменяет дистанцию до цели. А чем лучше передвигается наш агент, тем лучше он обучился.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Вот описание параметров:
trainer_type - отвечает за тип тренера, который используется для обучения
batch_size - этот параметр определяет количество шагов (состояние, действие, вознаграждение…) при обновлении параметра
buffer_size - это число элементов в буфере, из которого случайно выбирается по одному элементу на каждой итерации при перемешивании
learning_rate - влияет на то, в какой степени вновь полученная информация превосходит старую. Простыми словами параметр определяет скорость обучения методом градиентного спуска. Если значение слишком высокое, вознаграждение будет нестабильным
beta - этот параметр определяет интенсивность регуляризации энтропии, которая способствует изучению стратегий диверсификации. Проще говоря, регуляризация энтропии обычно используется для обогащения нашей функции стратегии, чтобы изучать различные стратегии и действия. Другими словами, чем больше бета, тем больше поощряется изучение
epsilon - этот параметр определяет, насколько быстро можно обновить политику
lambd - этот параметр определяет, насколько наш алгоритм зависит от предполагаемой величины вознаграждения и насколько он зависит от фактической величины вознаграждения в соответствии с официальной документацией
num_epoch - этот параметр определяет количество повторений каждого обновления градиентного спуска
learning_rate_schedule - этот параметр определяет, следует ли использовать метод снижения скорости обучения для стабилизации процесса обучения. Два дополнительных режима — линейное (Linear) и постоянное снижение (Constant). В первом случае скорость обучения будет линейно уменьшаться, а во втором — не меняться
max_steps - колличество шагов, которые выполнит обучение
time_horizon - этот параметр определяет, сколько шагов нужно сделать, чтобы начать помещать собранные данные об опыте в буфер опыта
num_layers - этот параметр определяет количество слоёв в обучающей сети. Чем больше число, тем глубже ваша модель
normalize - этот параметр определяет, будет ли модель нормализовать входной вектор наблюдений
hidden_units - этот параметр определяет количество нейронов в скрытом слое вашей обучающей сети или её размерность. Размер этого значения определяет способность нейронной сети выражать состояние игры
extrinsic - вознаграждения нашему агенту
strength - определяет интенсивность сигналов вознаграждения из внешней среды, получаемых моделью
gamma - этот параметр определяет коэффициент затухания долгосрочного вознаграждения. Проще говоря, если наша модель получила вознаграждение в размере 100 за один шаг, то сколько мы должны вознаградить за предыдущий шаг? Если за предыдущий шаг нет другого вознаграждения, то мы должны получить gamma * 100 = 99 за предыдущий шаг, gamma^2 * 100 за следующий шаг и так далее

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Я думаю, можно воспользоваться этими примерами для создания стратегии. Я имею ввиду какую-нибудь игру, где нужно добывать золото, чтобы улучшить замок и при этом сражаться с апонентом или вроде того. Особенно хорошо будет в таких играх, где есть множество уровней, поэтому не придёться прокладывать пути NPC на каждом уровне (NPC, которые добывают золото, можно будет убить и забрать таким образом золото врага, поэтому нужны именно NPC). 
МL-агента проще будет использовать для реализации следующих вещей:
- NPC – неигровые персонажи;
- Алгоритмы машинного обучения для улучшении качества графики;
- Для расширения игрового процесса;

## Выводы

Научились интегрировать в наш Unity проект ML-агента, который научился сам определять оптимальный маршрут до цели. В целом ML-агенты очень полезны и в целом машинное обучение облегчает работу разработчикам. Благодаря ним можно не только облегчить работу разработчикам, но и улучшить игровой контент, наполнив его NPC, и не только таким образом. 
Это была заключительная практика по Анализу данных в GameDev. Было всё очень интересно и увлекательно, мне правда понравилось делать все эти практики 

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
